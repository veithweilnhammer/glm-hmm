{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Ashwood NatNeuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from scipy.stats import bernoulli\n",
    "import json\n",
    "import os\n",
    "from oneibl.onelight import ONE\n",
    "\n",
    "one = ONE()\n",
    "\n",
    "def get_animal_name(eid):\n",
    "    # get session id:\n",
    "    raw_session_id = eid.split('Subjects/')[1]\n",
    "    # Get animal:\n",
    "    animal = raw_session_id.split('/')[0]\n",
    "    return animal\n",
    "\n",
    "\n",
    "def get_raw_data(eid):\n",
    "    print(eid)\n",
    "    # get session id:\n",
    "    raw_session_id = eid.split('Subjects/')[1]\n",
    "    # Get animal:\n",
    "    animal = raw_session_id.split('/')[0]\n",
    "    # replace '/' with dash in session ID\n",
    "    session_id = raw_session_id.replace('/', '-')\n",
    "    # hack to work with ONE:\n",
    "    current_dir = os.getcwd()\n",
    "    os.chdir(\"../../data/ibl/\")\n",
    "    # Get choice data, stim data and rewarded/not rewarded:\n",
    "    choice = one.load_dataset(eid, '_ibl_trials.choice')\n",
    "    stim_left = one.load_dataset(eid, '_ibl_trials.contrastLeft')\n",
    "    stim_right = one.load_dataset(eid, '_ibl_trials.contrastRight')\n",
    "    rewarded = one.load_dataset(eid, '_ibl_trials.feedbackType')\n",
    "    bias_probs = one.load_dataset(eid, '_ibl_trials.probabilityLeft')\n",
    "    os.chdir(current_dir)\n",
    "    return animal, session_id, stim_left, stim_right, rewarded, choice, \\\n",
    "           bias_probs\n",
    "\n",
    "\n",
    "def create_stim_vector(stim_left, stim_right):\n",
    "    # want stim_right - stim_left\n",
    "    # Replace NaNs with 0:\n",
    "    stim_left = np.nan_to_num(stim_left, nan=0)\n",
    "    stim_right = np.nan_to_num(stim_right, nan=0)\n",
    "    # now get 1D stim\n",
    "    signed_contrast = stim_right - stim_left\n",
    "    return signed_contrast\n",
    "\n",
    "\n",
    "def create_previous_choice_vector(choice):\n",
    "    ''' choice: choice vector of size T\n",
    "        previous_choice : vector of size T with previous choice made by\n",
    "        animal - output is in {0, 1}, where 0 corresponds to a previous left\n",
    "        choice; 1 corresponds to right.\n",
    "        If the previous choice was a violation, replace this with the choice\n",
    "        on the previous trial that was not a violation.\n",
    "        locs_mapping: array of size (~num_viols)x2, where the entry in\n",
    "        column 1 is the location in the previous choice vector that was a\n",
    "        remapping due to a violation and the\n",
    "        entry in column 2 is the location in the previous choice vector that\n",
    "        this location was remapped to\n",
    "    '''\n",
    "    previous_choice = np.hstack([np.array(choice[0]), choice])[:-1]\n",
    "    locs_to_update = np.where(previous_choice == -1)[0]\n",
    "    locs_with_choice = np.where(previous_choice != -1)[0]\n",
    "    loc_first_choice = locs_with_choice[0]\n",
    "    locs_mapping = np.zeros((len(locs_to_update) - loc_first_choice, 2),\n",
    "                            dtype='int')\n",
    "\n",
    "    for i, loc in enumerate(locs_to_update):\n",
    "        if loc < loc_first_choice:\n",
    "            # since no previous choice, bernoulli sample: (not output of\n",
    "            # bernoulli rvs is in {1, 2})\n",
    "            previous_choice[loc] = bernoulli.rvs(0.5, 1) - 1\n",
    "        else:\n",
    "            # find nearest loc that has a previous choice value that is not\n",
    "            # -1, and that is earlier than current trial\n",
    "            potential_matches = locs_with_choice[\n",
    "                np.where(locs_with_choice < loc)]\n",
    "            absolute_val_diffs = np.abs(loc - potential_matches)\n",
    "            absolute_val_diffs_ind = absolute_val_diffs.argmin()\n",
    "            nearest_loc = potential_matches[absolute_val_diffs_ind]\n",
    "            locs_mapping[i - loc_first_choice, 0] = int(loc)\n",
    "            locs_mapping[i - loc_first_choice, 1] = int(nearest_loc)\n",
    "            previous_choice[loc] = previous_choice[nearest_loc]\n",
    "    assert len(np.unique(\n",
    "        previous_choice)) <= 2, \"previous choice should be in {0, 1}; \" + str(\n",
    "        np.unique(previous_choice))\n",
    "    return previous_choice, locs_mapping\n",
    "\n",
    "\n",
    "def create_wsls_covariate(previous_choice, success, locs_mapping):\n",
    "    '''\n",
    "    inputs:\n",
    "    success: vector of size T, entries are in {-1, 1} and 0 corresponds to\n",
    "    failure, 1 corresponds to success\n",
    "    previous_choice: vector of size T, entries are in {0, 1} and 0\n",
    "    corresponds to left choice, 1 corresponds to right choice\n",
    "    locs_mapping: location remapping dictionary due to violations\n",
    "    output:\n",
    "    wsls: vector of size T, entries are in {-1, 1}.  1 corresponds to\n",
    "    previous choice = right and success OR previous choice = left and\n",
    "    failure; -1 corresponds to\n",
    "    previous choice = left and success OR previous choice = right and failure\n",
    "    '''\n",
    "    # remap previous choice vals to {-1, 1}\n",
    "    remapped_previous_choice = 2 * previous_choice - 1\n",
    "    previous_reward = np.hstack([np.array(success[0]), success])[:-1]\n",
    "    # Now need to go through and update previous reward to correspond to\n",
    "    # same trial as previous choice:\n",
    "    for i, loc in enumerate(locs_mapping[:, 0]):\n",
    "        nearest_loc = locs_mapping[i, 1]\n",
    "        previous_reward[loc] = previous_reward[nearest_loc]\n",
    "    wsls = previous_reward * remapped_previous_choice\n",
    "    assert len(np.unique(wsls)) == 2, \"wsls should be in {-1, 1}\"\n",
    "    return wsls\n",
    "\n",
    "\n",
    "def remap_choice_vals(choice):\n",
    "    # raw choice vector has CW = 1 (correct response for stim on left),\n",
    "    # CCW = -1 (correct response for stim on right) and viol = 0.  Let's\n",
    "    # remap so that CW = 0, CCw = 1, and viol = -1\n",
    "    choice_mapping = {1: 0, -1: 1, 0: -1}\n",
    "    new_choice_vector = [choice_mapping[old_choice] for old_choice in choice]\n",
    "    return new_choice_vector\n",
    "\n",
    "\n",
    "def create_design_mat(choice, stim_left, stim_right, rewarded):\n",
    "    # Create unnormalized_inpt: with first column = stim_right - stim_left,\n",
    "    # second column as past choice, third column as WSLS\n",
    "    stim = create_stim_vector(stim_left, stim_right)\n",
    "    T = len(stim)\n",
    "    design_mat = np.zeros((T, 3))\n",
    "    design_mat[:, 0] = stim\n",
    "    # make choice vector so that correct response for stim>0 is choice =1\n",
    "    # and is 0 for stim <0 (viol is mapped to -1)\n",
    "    choice = remap_choice_vals(choice)\n",
    "    # create past choice vector:\n",
    "    previous_choice, locs_mapping = create_previous_choice_vector(choice)\n",
    "    # create wsls vector:\n",
    "    wsls = create_wsls_covariate(previous_choice, rewarded, locs_mapping)\n",
    "    # map previous choice to {-1,1}\n",
    "    design_mat[:, 1] = 2 * previous_choice - 1\n",
    "    design_mat[:, 2] = wsls\n",
    "    return design_mat\n",
    "\n",
    "\n",
    "def get_all_unnormalized_data_this_session(eid):\n",
    "    # Load raw data\n",
    "    animal, session_id, stim_left, stim_right, rewarded, choice, bias_probs \\\n",
    "        = get_raw_data(eid)\n",
    "    # Subset choice and design_mat to 50-50 entries:\n",
    "    trials_to_study = np.where(bias_probs == 0.5)[0]\n",
    "    num_viols_50 = len(np.where(choice[trials_to_study] == 0)[0])\n",
    "    if num_viols_50 < 10:\n",
    "        # Create design mat = matrix of size T x 3, with entries for\n",
    "        # stim/past choice/wsls\n",
    "        unnormalized_inpt = create_design_mat(choice[trials_to_study],\n",
    "                                              stim_left[trials_to_study],\n",
    "                                              stim_right[trials_to_study],\n",
    "                                              rewarded[trials_to_study])\n",
    "        y = np.expand_dims(remap_choice_vals(choice[trials_to_study]), axis=1)\n",
    "        session = [session_id for i in range(y.shape[0])]\n",
    "        rewarded = np.expand_dims(rewarded[trials_to_study], axis=1)\n",
    "    else:\n",
    "        unnormalized_inpt = np.zeros((90, 3))\n",
    "        y = np.zeros((90, 1))\n",
    "        session = []\n",
    "        rewarded = np.zeros((90, 1))\n",
    "    return animal, unnormalized_inpt, y, session, num_viols_50, rewarded\n",
    "\n",
    "\n",
    "def load_animal_list(file):\n",
    "    container = np.load(file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    animal_list = data[0]\n",
    "    return animal_list\n",
    "\n",
    "\n",
    "def load_animal_eid_dict(file):\n",
    "    with open(file, 'r') as f:\n",
    "        animal_eid_dict = json.load(f)\n",
    "    return animal_eid_dict\n",
    "\n",
    "\n",
    "def load_data(animal_file):\n",
    "    container = np.load(animal_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    inpt = data[0]\n",
    "    y = data[1]\n",
    "    y = y.astype('int')\n",
    "    session = data[2]\n",
    "    return inpt, y, session\n",
    "\n",
    "\n",
    "def create_train_test_sessions(session, num_folds=5):\n",
    "    # create a session-fold lookup table\n",
    "    num_sessions = len(np.unique(session))\n",
    "    # Map sessions to folds:\n",
    "    unshuffled_folds = np.repeat(np.arange(num_folds),\n",
    "                                 np.ceil(num_sessions / num_folds))\n",
    "    shuffled_folds = npr.permutation(unshuffled_folds)[:num_sessions]\n",
    "    assert len(np.unique(\n",
    "        shuffled_folds)) == 5, \"require at least one session per fold for \" \\\n",
    "                               \"each animal!\"\n",
    "    # Look up table of shuffle-folds:\n",
    "    sess_id = np.array(np.unique(session), dtype='str')\n",
    "    shuffled_folds = np.array(shuffled_folds, dtype='O')\n",
    "    session_fold_lookup_table = np.transpose(\n",
    "        np.vstack([sess_id, shuffled_folds]))\n",
    "    return session_fold_lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download IBL dataset and begin processing it: identify unique animals in\n",
    "# IBL dataset that enter biased blocks.  Save a dictionary with each animal\n",
    "# and a list of their eids in the biased blocks\n",
    "\n",
    "import numpy as np\n",
    "from oneibl.onelight import ONE\n",
    "import numpy.random as npr\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "#from preprocessing_utils import get_animal_name\n",
    "npr.seed(65)\n",
    "\n",
    "DOWNLOAD_DATA = True # change to True to download raw data (WARNING: this\n",
    "# can take a while)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ibl_data_path = \"../../int-brain-lab/glm-hmm/data/ibl/\"\n",
    "    if DOWNLOAD_DATA: # Warning: this step takes a while\n",
    "        if not os.path.exists(ibl_data_path):\n",
    "            os.makedirs(ibl_data_path)\n",
    "        # download IBL data\n",
    "        url = 'https://ndownloader.figshare.com/files/21623715'\n",
    "        wget.download(url, ibl_data_path)\n",
    "        # now unzip downloaded data:\n",
    "        with ZipFile(ibl_data_path + \"ibl-behavior-data-Dec2019.zip\",\n",
    "                     'r') as zipObj:\n",
    "            # extract all the contents of zip file in ibl_data_path\n",
    "            zipObj.extractall(ibl_data_path)\n",
    "\n",
    "    # create directory for saving data:\n",
    "    if not os.path.exists(ibl_data_path + \"partially_processed/\"):\n",
    "        os.makedirs(ibl_data_path + \"partially_processed/\")\n",
    "\n",
    "    # change directory so that ONE searches in correct directory:\n",
    "    os.chdir(ibl_data_path)\n",
    "    one = ONE()\n",
    "    eids = one.search(['_ibl_trials.*'])\n",
    "    assert len(eids) > 0, \"ONE search is in incorrect directory\"\n",
    "    animal_list = []\n",
    "    animal_eid_dict = defaultdict(list)\n",
    "\n",
    "    for eid in eids:\n",
    "        bias_probs = one.load_dataset(eid, '_ibl_trials.probabilityLeft')\n",
    "        comparison = np.unique(bias_probs) == np.array([0.2, 0.5, 0.8])\n",
    "        # sessions with bias blocks\n",
    "        if isinstance(comparison, np.ndarray):\n",
    "            # update def of comparison to single True/False\n",
    "            comparison = comparison.all()\n",
    "        if comparison == True:\n",
    "            animal = get_animal_name(eid)\n",
    "            if animal not in animal_list:\n",
    "                animal_list.append(animal)\n",
    "            animal_eid_dict[animal].append(eid)\n",
    "\n",
    "    json = json.dumps(animal_eid_dict)\n",
    "    f = open(\"partially_processed/animal_eid_dict.json\",  \"w\")\n",
    "    f.write(json)\n",
    "    f.close()\n",
    "\n",
    "    np.savez('partially_processed/animal_list.npz', animal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue preprocessing of IBL dataset and create design matrix for GLM-HMM\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "#from preprocessing_utils import load_animal_list, load_animal_eid_dict, \\\n",
    "#    get_all_unnormalized_data_this_session, create_train_test_sessions\n",
    "\n",
    "npr.seed(65)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../../data/ibl/'\n",
    "    # Create directories for saving data:\n",
    "    processed_ibl_data_path = data_dir + \"data_for_cluster/\"\n",
    "    if not os.path.exists(processed_ibl_data_path):\n",
    "        os.makedirs(processed_ibl_data_path)\n",
    "    # Also create a subdirectory for storing each individual animal's data:\n",
    "    if not os.path.exists(processed_ibl_data_path + \"data_by_animal/\"):\n",
    "        os.makedirs(processed_ibl_data_path + \"data_by_animal/\")\n",
    "\n",
    "    # Load animal list/results of partial processing:\n",
    "    animal_list = load_animal_list(\n",
    "        data_dir + 'partially_processed/animal_list.npz')\n",
    "    animal_eid_dict = load_animal_eid_dict(\n",
    "        data_dir + 'partially_processed/animal_eid_dict.json')\n",
    "\n",
    "    # Require that each animal has at least 30 sessions (=2700 trials) of data:\n",
    "    req_num_sessions = 30  # 30*90 = 2700\n",
    "    for animal in animal_list:\n",
    "        num_sessions = len(animal_eid_dict[animal])\n",
    "        if num_sessions < req_num_sessions:\n",
    "            animal_list = np.delete(animal_list,\n",
    "                                    np.where(animal_list == animal))\n",
    "    # Identify idx in master array where each animal's data starts and ends:\n",
    "    animal_start_idx = {}\n",
    "    animal_end_idx = {}\n",
    "\n",
    "    final_animal_eid_dict = defaultdict(list)\n",
    "    # WORKHORSE: iterate through each animal and each animal's set of eids;\n",
    "    # obtain unnormalized data.  Write out each animal's data and then also\n",
    "    # write to master array\n",
    "    for z, animal in enumerate(animal_list):\n",
    "        sess_counter = 0\n",
    "        for eid in animal_eid_dict[animal]:\n",
    "            animal, unnormalized_inpt, y, session, num_viols_50, rewarded = \\\n",
    "                get_all_unnormalized_data_this_session(\n",
    "                    eid)\n",
    "            if num_viols_50 < 10:  # only include session if number of viols\n",
    "                # in 50-50 block is less than 10\n",
    "                if sess_counter == 0:\n",
    "                    animal_unnormalized_inpt = np.copy(unnormalized_inpt)\n",
    "                    animal_y = np.copy(y)\n",
    "                    animal_session = session\n",
    "                    animal_rewarded = np.copy(rewarded)\n",
    "                else:\n",
    "                    animal_unnormalized_inpt = np.vstack(\n",
    "                        (animal_unnormalized_inpt, unnormalized_inpt))\n",
    "                    animal_y = np.vstack((animal_y, y))\n",
    "                    animal_session = np.concatenate((animal_session, session))\n",
    "                    animal_rewarded = np.vstack((animal_rewarded, rewarded))\n",
    "                sess_counter += 1\n",
    "                final_animal_eid_dict[animal].append(eid)\n",
    "        # Write out animal's unnormalized data matrix:\n",
    "        np.savez(\n",
    "            processed_ibl_data_path + 'data_by_animal/' + animal +\n",
    "            '_unnormalized.npz',\n",
    "            animal_unnormalized_inpt, animal_y,\n",
    "            animal_session)\n",
    "        animal_session_fold_lookup = create_train_test_sessions(animal_session,\n",
    "                                                                5)\n",
    "        np.savez(\n",
    "            processed_ibl_data_path + 'data_by_animal/' + animal +\n",
    "            \"_session_fold_lookup\" +\n",
    "            \".npz\",\n",
    "            animal_session_fold_lookup)\n",
    "        np.savez(\n",
    "            processed_ibl_data_path + 'data_by_animal/' + animal +\n",
    "            '_rewarded.npz',\n",
    "            animal_rewarded)\n",
    "        assert animal_rewarded.shape[0] == animal_y.shape[0]\n",
    "        # Now create or append data to master array across all animals:\n",
    "        if z == 0:\n",
    "            master_inpt = np.copy(animal_unnormalized_inpt)\n",
    "            animal_start_idx[animal] = 0\n",
    "            animal_end_idx[animal] = master_inpt.shape[0] - 1\n",
    "            master_y = np.copy(animal_y)\n",
    "            master_session = animal_session\n",
    "            master_session_fold_lookup_table = animal_session_fold_lookup\n",
    "            master_rewarded = np.copy(animal_rewarded)\n",
    "        else:\n",
    "            animal_start_idx[animal] = master_inpt.shape[0]\n",
    "            master_inpt = np.vstack((master_inpt, animal_unnormalized_inpt))\n",
    "            animal_end_idx[animal] = master_inpt.shape[0] - 1\n",
    "            master_y = np.vstack((master_y, animal_y))\n",
    "            master_session = np.concatenate((master_session, animal_session))\n",
    "            master_session_fold_lookup_table = np.vstack(\n",
    "                (master_session_fold_lookup_table, animal_session_fold_lookup))\n",
    "            master_rewarded = np.vstack((master_rewarded, animal_rewarded))\n",
    "    # Write out data from across animals\n",
    "    assert np.shape(master_inpt)[0] == np.shape(master_y)[\n",
    "        0], \"inpt and y not same length\"\n",
    "    assert np.shape(master_rewarded)[0] == np.shape(master_y)[\n",
    "        0], \"rewarded and y not same length\"\n",
    "    assert len(np.unique(master_session)) == \\\n",
    "           np.shape(master_session_fold_lookup_table)[\n",
    "               0], \"number of unique sessions and session fold lookup don't \" \\\n",
    "                   \"match\"\n",
    "    assert len(master_inpt) == 181530, \"design matrix for all IBL animals \" \\\n",
    "                                       \"should have shape (181530, 3)\"\n",
    "    assert len(animal_list) == 37, \"37 animals were studied in Ashwood et \" \\\n",
    "                                   \"al. (2020)\"\n",
    "    normalized_inpt = np.copy(master_inpt)\n",
    "    normalized_inpt[:, 0] = preprocessing.scale(normalized_inpt[:, 0])\n",
    "    np.savez(processed_ibl_data_path + 'all_animals_concat' + '.npz',\n",
    "             normalized_inpt,\n",
    "             master_y, master_session)\n",
    "    np.savez(\n",
    "        processed_ibl_data_path + 'all_animals_concat_unnormalized' + '.npz',\n",
    "        master_inpt, master_y, master_session)\n",
    "    np.savez(\n",
    "        processed_ibl_data_path + 'all_animals_concat_session_fold_lookup' +\n",
    "        '.npz',\n",
    "        master_session_fold_lookup_table)\n",
    "    np.savez(processed_ibl_data_path + 'all_animals_concat_rewarded' + '.npz',\n",
    "             master_rewarded)\n",
    "    np.savez(processed_ibl_data_path + 'data_by_animal/' + 'animal_list.npz',\n",
    "             animal_list)\n",
    "\n",
    "    json = json.dumps(final_animal_eid_dict)\n",
    "    f = open(processed_ibl_data_path + \"final_animal_eid_dict.json\", \"w\")\n",
    "    f.write(json)\n",
    "    f.close()\n",
    "\n",
    "    # Now write out normalized data (when normalized across all animals) for\n",
    "    # each animal:\n",
    "    counter = 0\n",
    "    for animal in animal_start_idx.keys():\n",
    "        start_idx = animal_start_idx[animal]\n",
    "        end_idx = animal_end_idx[animal]\n",
    "        inpt = normalized_inpt[range(start_idx, end_idx + 1)]\n",
    "        y = master_y[range(start_idx, end_idx + 1)]\n",
    "        session = master_session[range(start_idx, end_idx + 1)]\n",
    "        counter += inpt.shape[0]\n",
    "        np.savez(processed_ibl_data_path + 'data_by_animal/' + animal + '_processed.npz',\n",
    "                 inpt, y,\n",
    "                 session)\n",
    "\n",
    "    assert counter == master_inpt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain IBL response time data for producing Figure 6\n",
    "# Write out the response times and the corresponding sessions\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from oneibl.onelight import ONE\n",
    "\n",
    "#from preprocessing_utils import load_animal_eid_dict, load_data\n",
    "\n",
    "npr.seed(65)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ibl_data_path = \"../../data/ibl/\"\n",
    "    animal_eid_dict = load_animal_eid_dict(\n",
    "        ibl_data_path + 'data_for_cluster/final_animal_eid_dict.json')\n",
    "    # must change directory for working with ONE\n",
    "    os.chdir(ibl_data_path)\n",
    "    one = ONE()\n",
    "\n",
    "    data_dir = 'response_times/data_by_animal/'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    for animal in animal_eid_dict.keys():\n",
    "        print(animal)\n",
    "        animal_inpt, animal_y, animal_session = load_data(\n",
    "            'data_for_cluster/data_by_animal/' + animal + '_processed.npz')\n",
    "        for z, eid in enumerate(animal_eid_dict[animal]):\n",
    "            raw_session_id = eid.split('Subjects/')[1]\n",
    "            session_id = raw_session_id.replace('/', '-')\n",
    "            full_sess_len = len(one.load_dataset(eid, '_ibl_trials.choice'))\n",
    "\n",
    "            file_names = [\n",
    "                '_ibl_trials.feedback_times', '_ibl_trials.response_times',\n",
    "                '_ibl_trials.goCue_times', '_ibl_trials.stimOn_times'\n",
    "            ]\n",
    "\n",
    "            save_vars = [\n",
    "                'feedback_times', 'response_times', 'go_cues', 'stim_on_times'\n",
    "            ]\n",
    "\n",
    "            for i, file in enumerate(file_names):\n",
    "                full_path = 'ibl-behavioral-data-Dec2019/' + eid + \\\n",
    "                                 '/alf/' + file + '.npy'\n",
    "                if os.path.exists(full_path):\n",
    "                    globals()[save_vars[i]] = one.load_dataset(eid, file)\n",
    "                else:\n",
    "                    globals()[save_vars[i]] = np.empty((full_sess_len, ))\n",
    "                    globals()[save_vars[i]][:] = np.nan\n",
    "\n",
    "            start = np.nanmin(np.c_[stim_on_times, go_cues], axis=1)\n",
    "\n",
    "            if (len(feedback_times) == len(response_times)): # some response\n",
    "                # times/feedback times are missing, so fill these as best as\n",
    "                # possible\n",
    "                end = np.nanmin(np.c_[feedback_times, response_times], axis=1)\n",
    "            elif len(feedback_times) == full_sess_len:\n",
    "                end = feedback_times\n",
    "            elif len(response_times) == full_sess_len:\n",
    "                end = response_times\n",
    "\n",
    "            # check timestamps increasing:\n",
    "            idx_to_change = np.where(start > end)[0]\n",
    "\n",
    "            if len(idx_to_change) > 0:\n",
    "                start[idx_to_change[0]] = np.nan\n",
    "                end[idx_to_change[0]] = np.nan\n",
    "\n",
    "            # Check we have times for at least some trials\n",
    "            nan_trial = np.isnan(np.c_[start, end]).any(axis=1)\n",
    "\n",
    "            is_increasing = (((start < end) | nan_trial).all() and\n",
    "                    ((np.diff(start) > 0) | np.isnan(\n",
    "                        np.diff(start))).all())\n",
    "\n",
    "            if is_increasing and ~nan_trial.all() and len(start) == \\\n",
    "                    full_sess_len and len(end) == full_sess_len: #\n",
    "                # check that times are increasing and that len(start) ==\n",
    "                # full_sess_len etc\n",
    "                prob_left_dta = one.load_dataset(\n",
    "                    eid, '_ibl_trials.probabilityLeft')\n",
    "                assert start.shape[0] == prob_left_dta.shape[0],\\\n",
    "                    \"different lengths for prob left and raw response dta: \" + \\\n",
    "                    str(start.shape[0]) + \" vs \" + str(\n",
    "                        prob_left_dta.shape[0])\n",
    "\n",
    "                # subset to trials corresponding to prob_left == 0.5:\n",
    "                unbiased_idx = np.where(prob_left_dta == 0.5)\n",
    "                response_dta = end[unbiased_idx] - start[unbiased_idx]\n",
    "\n",
    "                if ((np.nanmedian(response_dta) >= 10) | (np.nanmedian(\n",
    "                        response_dta) == np.nan)): # check that median\n",
    "                    # response time for session is less than 10 seconds\n",
    "                    response_dta = np.array([np.nan for i in range(len(\n",
    "                        unbiased_idx[0]))])\n",
    "\n",
    "                rt_sess = [session_id for i in range(response_dta.shape[0])]\n",
    "                # before saving, confirm that there are as many trials as in\n",
    "                # some of the other data:\n",
    "                assert len(rt_sess) == animal_inpt[np.where(animal_session ==\n",
    "                                                            session_id),\n",
    "                                       :].shape[1], \"response dta is different \" \\\n",
    "                                                    \"shape compared to inpt\"\n",
    "            else: # if any of the conditions above fail, fill the session's\n",
    "                # data with nans\n",
    "                len_prob_50 = animal_inpt[np.where(animal_session ==\n",
    "                                                            session_id),\n",
    "                              :].shape[1]\n",
    "                response_dta = np.array([np.nan for i in range(len_prob_50)])\n",
    "                rt_sess = [session_id for i in range(response_dta.shape[0])]\n",
    "\n",
    "            if z == 0:\n",
    "                rt_session_dta_this_animal = rt_sess\n",
    "                response_dta_this_animal = response_dta\n",
    "            else:\n",
    "                rt_session_dta_this_animal = np.concatenate(\n",
    "                    (rt_session_dta_this_animal, rt_sess))\n",
    "                response_dta_this_animal = np.concatenate(\n",
    "                    (response_dta_this_animal, response_dta))\n",
    "\n",
    "        assert len(response_dta_this_animal) == len(animal_inpt), \"different size for response times and inpt\"\n",
    "        np.savez(data_dir + animal + '.npz', response_dta_this_animal,\n",
    "                 rt_session_dta_this_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM class\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.scipy.special import logsumexp\n",
    "# Import useful functions from ssm package\n",
    "from ssm.util import ensure_args_are_lists\n",
    "from ssm.optimizers import adam, bfgs, rmsprop, sgd\n",
    "import ssm.stats as stats\n",
    "\n",
    "\n",
    "class glm(object):\n",
    "    def __init__(self, M, C):\n",
    "        \"\"\"\n",
    "        @param C:  number of classes in the categorical observations\n",
    "        \"\"\"\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        # Parameters linking input to state distribution\n",
    "        self.Wk = npr.randn(1, C - 1, M + 1)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.Wk\n",
    "\n",
    "    @params.setter\n",
    "    def params(self, value):\n",
    "        self.Wk = value\n",
    "\n",
    "    def log_prior(self):\n",
    "        return 0\n",
    "\n",
    "    # Calculate time dependent logits - output is matrix of size Tx1xC\n",
    "    # Input is size TxM\n",
    "    def calculate_logits(self, input):\n",
    "        # Update input to include offset term:\n",
    "        input = np.append(input, np.ones((input.shape[0], 1)), axis=1)\n",
    "        # Add additional row (of zeros) to second dimension of self.Wk\n",
    "        Wk_tranpose = np.transpose(self.Wk, (1, 0, 2))\n",
    "        Wk = np.transpose(\n",
    "            np.vstack([\n",
    "                Wk_tranpose,\n",
    "                np.zeros((1, Wk_tranpose.shape[1], Wk_tranpose.shape[2]))\n",
    "            ]), (1, 0, 2))\n",
    "        # Input effect; transpose so that output has dims TxKxC\n",
    "        time_dependent_logits = np.transpose(np.dot(Wk, input.T), (2, 0, 1))\n",
    "        time_dependent_logits = time_dependent_logits - logsumexp(\n",
    "            time_dependent_logits, axis=2, keepdims=True)\n",
    "        return time_dependent_logits\n",
    "\n",
    "    # Calculate log-likelihood of observed data\n",
    "    def log_likelihoods(self, data, input, mask, tag):\n",
    "        time_dependent_logits = self.calculate_logits(input)\n",
    "        mask = np.ones_like(data, dtype=bool) if mask is None else mask\n",
    "        return stats.categorical_logpdf(data[:, None, :],\n",
    "                                        time_dependent_logits[:, :, None, :],\n",
    "                                        mask=mask[:, None, :])\n",
    "\n",
    "    # log marginal likelihood of data\n",
    "    @ensure_args_are_lists\n",
    "    def log_marginal(self, datas, inputs, masks, tags):\n",
    "        elbo = self.log_prior()\n",
    "        for data, input, mask, tag in zip(datas, inputs, masks, tags):\n",
    "            lls = self.log_likelihoods(data, input, mask, tag)\n",
    "            elbo += np.sum(lls)\n",
    "        return elbo\n",
    "\n",
    "    @ensure_args_are_lists\n",
    "    def fit_glm(self,\n",
    "                datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                tags,\n",
    "                num_iters=1000,\n",
    "                optimizer=\"bfgs\",\n",
    "                **kwargs):\n",
    "        optimizer = dict(adam=adam, bfgs=bfgs, rmsprop=rmsprop,\n",
    "                         sgd=sgd)[optimizer]\n",
    "\n",
    "        def _objective(params, itr):\n",
    "            self.params = params\n",
    "            obj = self.log_marginal(datas, inputs, masks, tags)\n",
    "            return -obj\n",
    "\n",
    "        self.params = optimizer(_objective,\n",
    "                                self.params,\n",
    "                                num_iters=num_iters,\n",
    "                                **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "#from GLM import glm\n",
    "\n",
    "npr.seed(65)\n",
    "\n",
    "\n",
    "def load_data(animal_file):\n",
    "    container = np.load(animal_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    inpt = data[0]\n",
    "    y = data[1]\n",
    "    session = data[2]\n",
    "    return inpt, y, session\n",
    "\n",
    "\n",
    "def fit_glm(inputs, datas, M, C):\n",
    "    new_glm = glm(M, C)\n",
    "    new_glm.fit_glm(datas, inputs, masks=None, tags=None)\n",
    "    # Get loglikelihood of training data:\n",
    "    loglikelihood_train = new_glm.log_marginal(datas, inputs, None, None)\n",
    "    recovered_weights = new_glm.Wk\n",
    "    return loglikelihood_train, recovered_weights\n",
    "\n",
    "\n",
    "# Append column of zeros to weights matrix in appropriate location\n",
    "def append_zeros(weights):\n",
    "    weights_tranpose = np.transpose(weights, (1, 0, 2))\n",
    "    weights = np.transpose(\n",
    "        np.vstack([\n",
    "            weights_tranpose,\n",
    "            np.zeros((1, weights_tranpose.shape[1], weights_tranpose.shape[2]))\n",
    "        ]), (1, 0, 2))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def load_session_fold_lookup(file_path):\n",
    "    container = np.load(file_path, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    session_fold_lookup_table = data[0]\n",
    "    return session_fold_lookup_table\n",
    "\n",
    "\n",
    "def load_animal_list(list_file):\n",
    "    container = np.load(list_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    animal_list = data[0]\n",
    "    return animal_list\n",
    "\n",
    "\n",
    "def plot_input_vectors(Ws,\n",
    "                       figure_directory,\n",
    "                       title='true',\n",
    "                       save_title=\"true\",\n",
    "                       labels_for_plot=[]):\n",
    "    K = Ws.shape[0]\n",
    "    K_prime = Ws.shape[1]\n",
    "    M = Ws.shape[2] - 1\n",
    "    fig = plt.figure(figsize=(7, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.subplots_adjust(left=0.15,\n",
    "                        bottom=0.27,\n",
    "                        right=0.95,\n",
    "                        top=0.95,\n",
    "                        wspace=0.3,\n",
    "                        hspace=0.3)\n",
    "\n",
    "    for j in range(K):\n",
    "        for k in range(K_prime - 1):\n",
    "            # plt.subplot(K, K_prime, 1+j*K_prime+k)\n",
    "            plt.plot(range(M + 1), -Ws[j][k], marker='o')\n",
    "            plt.plot(range(-1, M + 2), np.repeat(0, M + 3), 'k', alpha=0.2)\n",
    "            plt.axhline(y=0, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "            if len(labels_for_plot) > 0:\n",
    "                plt.xticks(list(range(0, len(labels_for_plot))),\n",
    "                           labels_for_plot,\n",
    "                           rotation='90',\n",
    "                           fontsize=12)\n",
    "            else:\n",
    "                plt.xticks(list(range(0, 3)),\n",
    "                           ['Stimulus', 'Past Choice', 'Bias'],\n",
    "                           rotation='90',\n",
    "                           fontsize=12)\n",
    "            plt.ylim((-3, 6))\n",
    "\n",
    "    fig.text(0.04,\n",
    "             0.5,\n",
    "             \"Weight\",\n",
    "             ha=\"center\",\n",
    "             va=\"center\",\n",
    "             rotation=90,\n",
    "             fontsize=15)\n",
    "    fig.suptitle(\"GLM Weights: \" + title, y=0.99, fontsize=14)\n",
    "    fig.savefig(figure_directory + 'glm_weights_' + save_title + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM: all animals together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fit GLM to all IBL data together\n",
    "\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import os\n",
    "#from glm_utils import load_session_fold_lookup, load_data, fit_glm, \\\n",
    "#    plot_input_vectors, append_zeros\n",
    "\n",
    "C = 2  # number of output types/categories\n",
    "N_initializations = 10\n",
    "npr.seed(65)  # set seed in case of randomization\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../../int-brain-lab/glm-hmm/data/ibl/data_for_cluster/'\n",
    "    num_folds = 5\n",
    "\n",
    "    # Create directory for results:\n",
    "    results_dir = '../../int-brain-lab/glm-hmm/results/ibl_global_fit/'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Fit GLM to all data\n",
    "    animal_file = data_dir + 'all_animals_concat.npz'\n",
    "    inpt, y, session = load_data(animal_file)\n",
    "    session_fold_lookup_table = load_session_fold_lookup(\n",
    "        data_dir + 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "        # Subset to relevant covariates for covar set of interest:\n",
    "        labels_for_plot = ['stim', 'P_C', 'WSLS', 'bias']\n",
    "        y = y.astype('int')\n",
    "        figure_directory = results_dir + \"GLM/fold_\" + str(fold) + '/'\n",
    "        if not os.path.exists(figure_directory):\n",
    "            os.makedirs(figure_directory)\n",
    "\n",
    "        # Subset to sessions of interest for fold\n",
    "        sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "            session_fold_lookup_table[:, 1] != fold), 0]\n",
    "        idx_this_fold = [\n",
    "            str(sess) in sessions_to_keep and y[id, 0] != -1\n",
    "            for id, sess in enumerate(session)\n",
    "        ]\n",
    "        this_inpt, this_y, this_session = inpt[idx_this_fold, :], y[\n",
    "            idx_this_fold, :], session[idx_this_fold]\n",
    "        assert len(\n",
    "            np.unique(this_y)\n",
    "        ) == 2, \"choice vector should only include 2 possible values\"\n",
    "        train_size = this_inpt.shape[0]\n",
    "\n",
    "        M = this_inpt.shape[1]\n",
    "        loglikelihood_train_vector = []\n",
    "\n",
    "        for iter in range(N_initializations):  # GLM fitting should be\n",
    "            # independent of initialization, so fitting multiple\n",
    "            # initializations is a good way to check that everything is\n",
    "            # working correctly\n",
    "            loglikelihood_train, recovered_weights = fit_glm([this_inpt],\n",
    "                                                             [this_y], M, C)\n",
    "            weights_for_plotting = append_zeros(recovered_weights)\n",
    "            plot_input_vectors(weights_for_plotting,\n",
    "                               figure_directory,\n",
    "                               title=\"GLM fit; Final LL = \" +\n",
    "                               str(loglikelihood_train),\n",
    "                               save_title='init' + str(iter),\n",
    "                               labels_for_plot=labels_for_plot)\n",
    "            loglikelihood_train_vector.append(loglikelihood_train)\n",
    "            np.savez(\n",
    "                figure_directory + 'variables_of_interest_iter_' + str(iter) +\n",
    "                '.npz', loglikelihood_train, recovered_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM: single animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GLM to each IBL animal separately\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import os\n",
    "#from glm_utils import load_session_fold_lookup, load_data, load_animal_list, \\\n",
    "#    fit_glm, plot_input_vectors, append_zeros\n",
    "\n",
    "npr.seed(65)\n",
    "\n",
    "C = 2  # number of output types/categories\n",
    "N_initializations = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../../int-brain-lab/glm-hmm/data/ibl/data_for_cluster/data_by_animal/'\n",
    "    num_folds = 5\n",
    "    animal_list = load_animal_list(data_dir + 'animal_list.npz')\n",
    "\n",
    "    results_dir = '../../int-brain-lab/glm-hmm/results/ibl_individual_fit/'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    for animal in animal_list:\n",
    "        # Fit GLM to data from single animal:\n",
    "        animal_file = data_dir + animal + '_processed.npz'\n",
    "        session_fold_lookup_table = load_session_fold_lookup(\n",
    "            data_dir + animal + '_session_fold_lookup.npz')\n",
    "\n",
    "        for fold in range(num_folds):\n",
    "            this_results_dir = results_dir + animal + '/'\n",
    "\n",
    "            # Load data\n",
    "            inpt, y, session = load_data(animal_file)\n",
    "            labels_for_plot = ['stim', 'pc', 'wsls', 'bias']\n",
    "            y = y.astype('int')\n",
    "\n",
    "            figure_directory = this_results_dir + \"GLM/fold_\" + str(fold) + '/'\n",
    "            if not os.path.exists(figure_directory):\n",
    "                os.makedirs(figure_directory)\n",
    "\n",
    "            # Subset to sessions of interest for fold\n",
    "            sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "                session_fold_lookup_table[:, 1] != fold), 0]\n",
    "            idx_this_fold = [\n",
    "                str(sess) in sessions_to_keep and y[id, 0] != -1\n",
    "                for id, sess in enumerate(session)\n",
    "            ]\n",
    "            this_inpt, this_y, this_session = inpt[idx_this_fold, :], \\\n",
    "                                              y[idx_this_fold, :], \\\n",
    "                                              session[idx_this_fold]\n",
    "            assert len(\n",
    "                np.unique(this_y)\n",
    "            ) == 2, \"choice vector should only include 2 possible values\"\n",
    "            train_size = this_inpt.shape[0]\n",
    "\n",
    "            M = this_inpt.shape[1]\n",
    "            loglikelihood_train_vector = []\n",
    "\n",
    "            for iter in range(N_initializations):\n",
    "                loglikelihood_train, recovered_weights = fit_glm([this_inpt],\n",
    "                                                                 [this_y], M,\n",
    "                                                                 C)\n",
    "                weights_for_plotting = append_zeros(recovered_weights)\n",
    "                plot_input_vectors(weights_for_plotting,\n",
    "                                   figure_directory,\n",
    "                                   title=\"GLM fit; Final LL = \" +\n",
    "                                   str(loglikelihood_train),\n",
    "                                   save_title='init' + str(iter),\n",
    "                                   labels_for_plot=labels_for_plot)\n",
    "                loglikelihood_train_vector.append(loglikelihood_train)\n",
    "                np.savez(\n",
    "                    figure_directory + 'variables_of_interest_iter_' +\n",
    "                    str(iter) + '.npz', loglikelihood_train, recovered_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM-HMM\n",
    "\n",
    "### Global GLM-HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to assist with GLM-HMM model fitting\n",
    "import sys\n",
    "import ssm\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "\n",
    "def load_data(animal_file):\n",
    "    container = np.load(animal_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    inpt = data[0]\n",
    "    y = data[1]\n",
    "    session = data[2]\n",
    "    return inpt, y, session\n",
    "\n",
    "\n",
    "def load_cluster_arr(cluster_arr_file):\n",
    "    container = np.load(cluster_arr_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    cluster_arr = data[0]\n",
    "    return cluster_arr\n",
    "\n",
    "\n",
    "def load_glm_vectors(glm_vectors_file):\n",
    "    container = np.load(glm_vectors_file)\n",
    "    data = [container[key] for key in container]\n",
    "    loglikelihood_train = data[0]\n",
    "    recovered_weights = data[1]\n",
    "    return loglikelihood_train, recovered_weights\n",
    "\n",
    "\n",
    "def load_global_params(global_params_file):\n",
    "    container = np.load(global_params_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    global_params = data[0]\n",
    "    return global_params\n",
    "\n",
    "\n",
    "def partition_data_by_session(inpt, y, mask, session):\n",
    "    '''\n",
    "    Partition inpt, y, mask by session\n",
    "    :param inpt: arr of size TxM\n",
    "    :param y:  arr of size T x D\n",
    "    :param mask: Boolean arr of size T indicating if element is violation or\n",
    "    not\n",
    "    :param session: list of size T containing session ids\n",
    "    :return: list of inpt arrays, data arrays and mask arrays, where the\n",
    "    number of elements in list = number of sessions and each array size is\n",
    "    number of trials in session\n",
    "    '''\n",
    "    inputs = []\n",
    "    datas = []\n",
    "    indexes = np.unique(session, return_index=True)[1]\n",
    "    unique_sessions = [session[index] for index in sorted(indexes)]\n",
    "    counter = 0\n",
    "    masks = []\n",
    "    for sess in unique_sessions:\n",
    "        idx = np.where(session == sess)[0]\n",
    "        counter += len(idx)\n",
    "        inputs.append(inpt[idx, :])\n",
    "        datas.append(y[idx, :])\n",
    "        masks.append(mask[idx, :])\n",
    "    assert counter == inpt.shape[0], \"not all trials assigned to session!\"\n",
    "    return inputs, datas, masks\n",
    "\n",
    "\n",
    "def load_session_fold_lookup(file_path):\n",
    "    container = np.load(file_path, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    session_fold_lookup_table = data[0]\n",
    "    return session_fold_lookup_table\n",
    "\n",
    "\n",
    "def load_animal_list(file):\n",
    "    container = np.load(file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    animal_list = data[0]\n",
    "    return animal_list\n",
    "\n",
    "\n",
    "def launch_glm_hmm_job(inpt, y, session, mask, session_fold_lookup_table, K, D,\n",
    "                       C, N_em_iters, transition_alpha, prior_sigma, fold,\n",
    "                       iter, global_fit, init_param_file, save_directory):\n",
    "    print(\"Starting inference with K = \" + str(K) + \"; Fold = \" + str(fold) +\n",
    "          \"; Iter = \" + str(iter))\n",
    "    sys.stdout.flush()\n",
    "    sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_this_fold = [str(sess) in sessions_to_keep for sess in session]\n",
    "    this_inpt, this_y, this_session, this_mask = inpt[idx_this_fold, :], \\\n",
    "                                                 y[idx_this_fold, :], \\\n",
    "                                                 session[idx_this_fold], \\\n",
    "                                                 mask[idx_this_fold]\n",
    "    # Only do this so that errors are avoided - these y values will not\n",
    "    # actually be used for anything (due to violation mask)\n",
    "    this_y[np.where(this_y == -1), :] = 1\n",
    "    inputs, datas, masks = partition_data_by_session(\n",
    "        this_inpt, this_y, this_mask, this_session)\n",
    "    # Read in GLM fit if global_fit = True:\n",
    "    if global_fit == True:\n",
    "        _, params_for_initialization = load_glm_vectors(init_param_file)\n",
    "    else:\n",
    "        params_for_initialization = load_global_params(init_param_file)\n",
    "    M = this_inpt.shape[1]\n",
    "    npr.seed(iter)\n",
    "    fit_glm_hmm(datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                K,\n",
    "                D,\n",
    "                M,\n",
    "                C,\n",
    "                N_em_iters,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                global_fit,\n",
    "                params_for_initialization,\n",
    "                save_title=save_directory + 'glm_hmm_raw_parameters_itr_' +\n",
    "                           str(iter) + '.npz')\n",
    "\n",
    "\n",
    "def fit_glm_hmm(datas, inputs, masks, K, D, M, C, N_em_iters,\n",
    "                transition_alpha, prior_sigma, global_fit,\n",
    "                params_for_initialization, save_title):\n",
    "    '''\n",
    "    Instantiate and fit GLM-HMM model\n",
    "    :param datas:\n",
    "    :param inputs:\n",
    "    :param masks:\n",
    "    :param K:\n",
    "    :param D:\n",
    "    :param M:\n",
    "    :param C:\n",
    "    :param N_em_iters:\n",
    "    :param global_fit:\n",
    "    :param glm_vectors:\n",
    "    :param save_title:\n",
    "    :return:\n",
    "    '''\n",
    "    if global_fit == True:\n",
    "        # Prior variables\n",
    "        # Choice of prior\n",
    "        this_hmm = ssm.HMM(K,\n",
    "                           D,\n",
    "                           M,\n",
    "                           observations=\"input_driven_obs\",\n",
    "                           observation_kwargs=dict(C=C,\n",
    "                                                   prior_sigma=prior_sigma),\n",
    "                           transitions=\"sticky\",\n",
    "                           transition_kwargs=dict(alpha=transition_alpha,\n",
    "                                                  kappa=0))\n",
    "        # Initialize observation weights as GLM weights with some noise:\n",
    "        glm_vectors_repeated = np.tile(params_for_initialization, (K, 1, 1))\n",
    "        glm_vectors_with_noise = glm_vectors_repeated + np.random.normal(\n",
    "            0, 0.2, glm_vectors_repeated.shape)\n",
    "        this_hmm.observations.params = glm_vectors_with_noise\n",
    "    else:\n",
    "        # Choice of prior\n",
    "        this_hmm = ssm.HMM(K,\n",
    "                           D,\n",
    "                           M,\n",
    "                           observations=\"input_driven_obs\",\n",
    "                           observation_kwargs=dict(C=C,\n",
    "                                                   prior_sigma=prior_sigma),\n",
    "                           transitions=\"sticky\",\n",
    "                           transition_kwargs=dict(alpha=transition_alpha,\n",
    "                                                  kappa=0))\n",
    "        # Initialize HMM-GLM with global parameters:\n",
    "        this_hmm.params = params_for_initialization\n",
    "        # Get log_prior of transitions:\n",
    "    print(\"=== fitting GLM-HMM ========\")\n",
    "    sys.stdout.flush()\n",
    "    # Fit this HMM and calculate marginal likelihood\n",
    "    lls = this_hmm.fit(datas,\n",
    "                       inputs=inputs,\n",
    "                       masks=masks,\n",
    "                       method=\"em\",\n",
    "                       num_iters=N_em_iters,\n",
    "                       initialize=False,\n",
    "                       tolerance=10 ** -4)\n",
    "    # Save raw parameters of HMM, as well as loglikelihood during training\n",
    "    np.savez(save_title, this_hmm.params, lls)\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_violation_mask(violation_idx, T):\n",
    "    \"\"\"\n",
    "    Return indices of nonviolations and also a Boolean mask for inclusion (1\n",
    "    = nonviolation; 0 = violation)\n",
    "    :param test_idx:\n",
    "    :param T:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mask = np.array([i not in violation_idx for i in range(T)])\n",
    "    nonviolation_idx = np.arange(T)[mask]\n",
    "    mask = mask + 0\n",
    "    assert len(nonviolation_idx) + len(\n",
    "        violation_idx\n",
    "    ) == T, \"violation and non-violation idx do not include all dta!\"\n",
    "    return nonviolation_idx, np.expand_dims(mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Functions to assist with post-processing of GLM-HMM fits\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ssm\n",
    "\n",
    "sys.path.insert(0, '../fit_glm/')\n",
    "sys.path.insert(0, '../fit_lapse_model/')\n",
    "#from GLM import glm\n",
    "#from LapseModel import lapse_model\n",
    "\n",
    "\n",
    "def load_data(animal_file):\n",
    "    container = np.load(animal_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    inpt = data[0]\n",
    "    y = data[1]\n",
    "    y = y.astype('int')\n",
    "    session = data[2]\n",
    "    return inpt, y, session\n",
    "\n",
    "\n",
    "def load_session_fold_lookup(file_path):\n",
    "    container = np.load(file_path, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    session_fold_lookup_table = data[0]\n",
    "    return session_fold_lookup_table\n",
    "\n",
    "\n",
    "def load_glm_vectors(glm_vectors_file):\n",
    "    container = np.load(glm_vectors_file)\n",
    "    data = [container[key] for key in container]\n",
    "    loglikelihood_train = data[0]\n",
    "    recovered_weights = data[1]\n",
    "    return loglikelihood_train, recovered_weights\n",
    "\n",
    "\n",
    "def load_lapse_params(lapse_file):\n",
    "    container = np.load(lapse_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    lapse_loglikelihood = data[0]\n",
    "    lapse_glm_weights = data[1]\n",
    "    lapse_glm_weights_std = data[2],\n",
    "    lapse_p = data[3]\n",
    "    lapse_p_std = data[4]\n",
    "    return lapse_loglikelihood, lapse_glm_weights, lapse_glm_weights_std, \\\n",
    "           lapse_p, lapse_p_std\n",
    "\n",
    "\n",
    "def load_glmhmm_data(data_file):\n",
    "    container = np.load(data_file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    this_hmm_params = data[0]\n",
    "    lls = data[1]\n",
    "    return [this_hmm_params, lls]\n",
    "\n",
    "\n",
    "def load_cv_arr(file):\n",
    "    container = np.load(file, allow_pickle=True)\n",
    "    data = [container[key] for key in container]\n",
    "    cvbt_folds_model = data[0]\n",
    "    return cvbt_folds_model\n",
    "\n",
    "\n",
    "def partition_data_by_session(inpt, y, mask, session):\n",
    "    '''\n",
    "    Partition inpt, y, mask by session\n",
    "    :param inpt: arr of size TxM\n",
    "    :param y:  arr of size T x D\n",
    "    :param mask: Boolean arr of size T indicating if element is violation or\n",
    "    not\n",
    "    :param session: list of size T containing session ids\n",
    "    :return: list of inpt arrays, data arrays and mask arrays, where the\n",
    "    number of elements in list = number of sessions and each array size is\n",
    "    number of trials in session\n",
    "    '''\n",
    "    inputs = []\n",
    "    datas = []\n",
    "    indexes = np.unique(session, return_index=True)[1]\n",
    "    unique_sessions = [\n",
    "        session[index] for index in sorted(indexes)\n",
    "    ]  # ensure that unique sessions are ordered as they are in\n",
    "    # session (so we can map inputs back to inpt)\n",
    "    counter = 0\n",
    "    masks = []\n",
    "    for sess in unique_sessions:\n",
    "        idx = np.where(session == sess)[0]\n",
    "        counter += len(idx)\n",
    "        inputs.append(inpt[idx, :])\n",
    "        datas.append(y[idx, :])\n",
    "        masks.append(mask[idx])\n",
    "    assert counter == inpt.shape[0], \"not all trials assigned to session!\"\n",
    "    return inputs, datas, masks\n",
    "\n",
    "\n",
    "def get_train_test_dta(inpt, y, mask, session, session_fold_lookup_table,\n",
    "                       fold):\n",
    "    '''\n",
    "    Split inpt, y, mask, session arrays into train and test arrays\n",
    "    :param inpt:\n",
    "    :param y:\n",
    "    :param mask:\n",
    "    :param session:\n",
    "    :param session_fold_lookup_table:\n",
    "    :param fold:\n",
    "    :return:\n",
    "    '''\n",
    "    test_sessions = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] == fold), 0]\n",
    "    train_sessions = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_test = [str(sess) in test_sessions for sess in session]\n",
    "    idx_train = [str(sess) in train_sessions for sess in session]\n",
    "    test_inpt, test_y, test_mask, this_test_session = inpt[idx_test, :], y[\n",
    "                                                                         idx_test,\n",
    "                                                                         :], \\\n",
    "                                                      mask[idx_test], session[\n",
    "                                                          idx_test]\n",
    "    train_inpt, train_y, train_mask, this_train_session = inpt[idx_train,\n",
    "                                                          :], y[idx_train,\n",
    "                                                              :], \\\n",
    "                                                          mask[idx_train], \\\n",
    "                                                          session[idx_train]\n",
    "    return test_inpt, test_y, test_mask, this_test_session, train_inpt, \\\n",
    "           train_y, train_mask, this_train_session\n",
    "\n",
    "\n",
    "def create_violation_mask(violation_idx, T):\n",
    "    \"\"\"\n",
    "    Return indices of nonviolations and also a Boolean mask for inclusion (1\n",
    "    = nonviolation; 0 = violation)\n",
    "    :param test_idx:\n",
    "    :param T:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mask = np.array([i not in violation_idx for i in range(T)])\n",
    "    nonviolation_idx = np.arange(T)[mask]\n",
    "    mask = mask + 0\n",
    "    assert len(nonviolation_idx) + len(\n",
    "        violation_idx) == T, \"violation and non-violation idx do not include \" \\\n",
    "                             \"\" \\\n",
    "                             \"\" \\\n",
    "                             \"\" \\\n",
    "                             \"\" \\\n",
    "                             \"all dta!\"\n",
    "    return nonviolation_idx, mask\n",
    "\n",
    "\n",
    "def prepare_data_for_cv(inpt, y, session, session_fold_lookup_table, fold):\n",
    "    '''\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    violation_idx = np.where(y == -1)[0]\n",
    "    nonviolation_idx, nonviolation_mask = create_violation_mask(\n",
    "        violation_idx, inpt.shape[0])\n",
    "    # Load train and test data for session\n",
    "    test_inpt, test_y, test_nonviolation_mask, this_test_session, \\\n",
    "    train_inpt, train_y, train_nonviolation_mask, this_train_session = \\\n",
    "        get_train_test_dta(\n",
    "            inpt, y, nonviolation_mask, session, session_fold_lookup_table,\n",
    "            fold)\n",
    "    M = train_inpt.shape[1]\n",
    "    n_test = np.sum(test_nonviolation_mask == 1)\n",
    "    n_train = np.sum(train_nonviolation_mask == 1)\n",
    "    return test_inpt, test_y, test_nonviolation_mask, this_test_session, \\\n",
    "           train_inpt, train_y, train_nonviolation_mask, this_train_session, \\\n",
    "           M, n_test, n_train\n",
    "\n",
    "\n",
    "def calculate_baseline_test_ll(train_y, test_y, C):\n",
    "    \"\"\"\n",
    "    Calculate baseline loglikelihood for CV bit/trial calculation.  This is\n",
    "    log(p(y|p0)) = n_right(log(p0)) + (n_total-n_right)log(1-p0), where p0\n",
    "    is the proportion of trials\n",
    "    in which the animal went right in the training set and n_right is the\n",
    "    number of trials in which the animal went right in the test set\n",
    "    :param train_y\n",
    "    :param test_y\n",
    "    :return: baseline loglikelihood for CV bit/trial calculation\n",
    "    \"\"\"\n",
    "    _, train_class_totals = np.unique(train_y, return_counts=True)\n",
    "    train_class_probs = train_class_totals / train_y.shape[0]\n",
    "    _, test_class_totals = np.unique(test_y, return_counts=True)\n",
    "    ll0 = 0\n",
    "    for c in range(C):\n",
    "        ll0 += test_class_totals[c] * np.log(train_class_probs[c])\n",
    "    return ll0\n",
    "\n",
    "\n",
    "def calculate_glm_test_loglikelihood(glm_weights_file, test_y, test_inpt, M,\n",
    "                                     C):\n",
    "    loglikelihood_train, glm_vectors = load_glm_vectors(glm_weights_file)\n",
    "    # Calculate test loglikelihood\n",
    "    new_glm = glm(M, C)\n",
    "    # Set parameters to fit parameters:\n",
    "    new_glm.params = glm_vectors\n",
    "    # Get loglikelihood of training data:\n",
    "    loglikelihood_test = new_glm.log_marginal([test_y], [test_inpt], None,\n",
    "                                              None)\n",
    "    return loglikelihood_test\n",
    "\n",
    "\n",
    "def calculate_lapse_test_loglikelihood(lapse_file, test_y, test_inpt, M,\n",
    "                                       num_lapse_params):\n",
    "    lapse_loglikelihood, lapse_glm_weights, _, lapse_p, _ = load_lapse_params(\n",
    "        lapse_file)\n",
    "    # Instantiate a model with these parameters\n",
    "    new_lapse_model = lapse_model(M, num_lapse_params)\n",
    "    if num_lapse_params == 1:\n",
    "        new_lapse_model.params = [lapse_glm_weights, np.array([lapse_p])]\n",
    "    else:\n",
    "        new_lapse_model.params = [lapse_glm_weights, lapse_p]\n",
    "    # Now calculate test loglikelihood\n",
    "    loglikelihood_test = new_lapse_model.log_marginal(datas=[test_y],\n",
    "                                                      inputs=[test_inpt],\n",
    "                                                      masks=None,\n",
    "                                                      tags=None)\n",
    "    return loglikelihood_test\n",
    "\n",
    "\n",
    "def return_lapse_nll(inpt, y, session, session_fold_lookup_table, fold,\n",
    "                     num_lapse_params, results_dir_glm_lapse, C):\n",
    "    test_inpt, test_y, test_nonviolation_mask, this_test_session, \\\n",
    "    train_inpt, train_y, train_nonviolation_mask, this_train_session, M, \\\n",
    "    n_test, n_train = prepare_data_for_cv(\n",
    "        inpt, y, session, session_fold_lookup_table, fold)\n",
    "    ll0 = calculate_baseline_test_ll(train_y[train_nonviolation_mask == 1, :],\n",
    "                                     test_y[test_nonviolation_mask == 1, :], C)\n",
    "    ll0_train = calculate_baseline_test_ll(\n",
    "        train_y[train_nonviolation_mask == 1, :],\n",
    "        train_y[train_nonviolation_mask == 1, :], C)\n",
    "    if num_lapse_params == 1:\n",
    "        lapse_file = results_dir_glm_lapse + '/Lapse_Model/fold_' + str(\n",
    "            fold) + '/lapse_model_params_one_param.npz'\n",
    "    elif num_lapse_params == 2:\n",
    "        lapse_file = results_dir_glm_lapse + '/Lapse_Model/fold_' + str(\n",
    "            fold) + '/lapse_model_params_two_param.npz'\n",
    "    ll_lapse = calculate_lapse_test_loglikelihood(\n",
    "        lapse_file,\n",
    "        test_y[test_nonviolation_mask == 1, :],\n",
    "        test_inpt[test_nonviolation_mask == 1, :],\n",
    "        M,\n",
    "        num_lapse_params=num_lapse_params)\n",
    "    ll_train_lapse = calculate_lapse_test_loglikelihood(\n",
    "        lapse_file,\n",
    "        train_y[train_nonviolation_mask == 1, :],\n",
    "        train_inpt[train_nonviolation_mask == 1, :],\n",
    "        M,\n",
    "        num_lapse_params=num_lapse_params)\n",
    "    nll_lapse = calculate_cv_bit_trial(ll_lapse, ll0, n_test)\n",
    "    nll_lapse_train = calculate_cv_bit_trial(ll_train_lapse, ll0_train,\n",
    "                                             n_train)\n",
    "    return nll_lapse, nll_lapse_train, ll_lapse, ll_train_lapse\n",
    "\n",
    "\n",
    "def calculate_glm_hmm_test_loglikelihood(glm_hmm_dir, test_datas, test_inputs,\n",
    "                                         test_nonviolation_masks, K, D, M, C):\n",
    "    \"\"\"\n",
    "    calculate test loglikelihood for GLM-HMM model.  Loop through all\n",
    "    initializations for fold of interest, and check that final train LL is\n",
    "    same for top initializations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    this_file_name = glm_hmm_dir + '/iter_*/glm_hmm_raw_parameters_*.npz'\n",
    "    raw_files = glob.glob(this_file_name, recursive=True)\n",
    "    train_ll_vals_across_iters = []\n",
    "    test_ll_vals_across_iters = []\n",
    "    for file in raw_files:\n",
    "        # Loop through initializations and calculate BIC:\n",
    "        this_hmm_params, lls = load_glmhmm_data(file)\n",
    "        train_ll_vals_across_iters.append(lls[-1])\n",
    "        # Instantiate a new HMM and calculate test loglikelihood:\n",
    "        this_hmm = ssm.HMM(K,\n",
    "                           D,\n",
    "                           M,\n",
    "                           observations=\"input_driven_obs\",\n",
    "                           observation_kwargs=dict(C=C),\n",
    "                           transitions=\"standard\")\n",
    "        this_hmm.params = this_hmm_params\n",
    "        test_ll = this_hmm.log_likelihood(test_datas,\n",
    "                                          inputs=test_inputs,\n",
    "                                          masks=test_nonviolation_masks)\n",
    "        test_ll_vals_across_iters.append(test_ll)\n",
    "    # Order initializations by train LL (don't train on test data!):\n",
    "    train_ll_vals_across_iters = np.array(train_ll_vals_across_iters)\n",
    "    test_ll_vals_across_iters = np.array(test_ll_vals_across_iters)\n",
    "    # Order raw files by train LL\n",
    "    file_ordering_by_train = np.argsort(-train_ll_vals_across_iters)\n",
    "    raw_file_ordering_by_train = np.array(raw_files)[file_ordering_by_train]\n",
    "    # Get initialization number from raw_file ordering\n",
    "    init_ordering_by_train = [\n",
    "        int(re.findall(r'\\d+', file)[-1])\n",
    "        for file in raw_file_ordering_by_train\n",
    "    ]\n",
    "    return test_ll_vals_across_iters, init_ordering_by_train, \\\n",
    "           file_ordering_by_train\n",
    "\n",
    "\n",
    "def return_glmhmm_nll(inpt, y, session, session_fold_lookup_table, fold, K, D,\n",
    "                      C, results_dir_glm_hmm):\n",
    "    '''\n",
    "    For a given fold, return NLL for both train and test datasets for\n",
    "    GLM-HMM model with K, D, C.  Requires reading in best\n",
    "    parameters over all initializations for GLM-HMM (hence why\n",
    "    results_dir_glm_hmm is required as an input)\n",
    "    :param inpt:\n",
    "    :param y:\n",
    "    :param session:\n",
    "    :param session_fold_lookup_table:\n",
    "    :param fold:\n",
    "    :param K:\n",
    "    :param D:\n",
    "    :param C:\n",
    "    :param results_dir_glm_hmm:\n",
    "    :return:\n",
    "    '''\n",
    "    test_inpt, test_y, test_nonviolation_mask, this_test_session, \\\n",
    "    train_inpt, train_y, train_nonviolation_mask, this_train_session, M, \\\n",
    "    n_test, n_train = prepare_data_for_cv(\n",
    "        inpt, y, session, session_fold_lookup_table, fold)\n",
    "    ll0 = calculate_baseline_test_ll(train_y[train_nonviolation_mask == 1, :],\n",
    "                                     test_y[test_nonviolation_mask == 1, :], C)\n",
    "    ll0_train = calculate_baseline_test_ll(\n",
    "        train_y[train_nonviolation_mask == 1, :],\n",
    "        train_y[train_nonviolation_mask == 1, :], C)\n",
    "    # For GLM-HMM set values of y for violations to 1.  This value doesn't\n",
    "    # matter (as mask will ensure that these y values do not contribute to\n",
    "    # loglikelihood calculation\n",
    "    test_y[test_nonviolation_mask == 0, :] = 1\n",
    "    train_y[train_nonviolation_mask == 0, :] = 1\n",
    "    # For GLM-HMM, need to partition data by session\n",
    "    test_inputs, test_datas, test_nonviolation_masks = \\\n",
    "        partition_data_by_session(\n",
    "            test_inpt, test_y,\n",
    "            np.expand_dims(test_nonviolation_mask, axis=1),\n",
    "            this_test_session)\n",
    "    train_inputs, train_datas, train_nonviolation_masks = \\\n",
    "        partition_data_by_session(\n",
    "            train_inpt, train_y,\n",
    "            np.expand_dims(train_nonviolation_mask, axis=1),\n",
    "            this_train_session)\n",
    "    dir_to_check = results_dir_glm_hmm + '/GLM_HMM_K_' + str(\n",
    "        K) + '/fold_' + str(fold) + '/'\n",
    "    test_ll_vals_across_iters, init_ordering_by_train, \\\n",
    "    file_ordering_by_train = calculate_glm_hmm_test_loglikelihood(\n",
    "        dir_to_check, test_datas, test_inputs, test_nonviolation_masks, K, D,\n",
    "        M, C)\n",
    "    train_ll_vals_across_iters, _, _ = calculate_glm_hmm_test_loglikelihood(\n",
    "        dir_to_check, train_datas, train_inputs, train_nonviolation_masks, K,\n",
    "        D, M, C)\n",
    "    test_ll_vals_across_iters = test_ll_vals_across_iters[\n",
    "        file_ordering_by_train]\n",
    "    train_ll_vals_across_iters = train_ll_vals_across_iters[\n",
    "        file_ordering_by_train]\n",
    "    ll_glm_hmm_this_K = test_ll_vals_across_iters[0]\n",
    "    cvbt_thismodel_thisfold = calculate_cv_bit_trial(ll_glm_hmm_this_K, ll0,\n",
    "                                                     n_test)\n",
    "    train_cvbt_thismodel_thisfold = calculate_cv_bit_trial(\n",
    "        train_ll_vals_across_iters[0], ll0_train, n_train)\n",
    "    return cvbt_thismodel_thisfold, train_cvbt_thismodel_thisfold, \\\n",
    "           ll_glm_hmm_this_K, \\\n",
    "           train_ll_vals_across_iters[0], init_ordering_by_train\n",
    "\n",
    "\n",
    "def calculate_cv_bit_trial(ll_model, ll_0, n_trials):\n",
    "    cv_bit_trial = ((ll_model - ll_0) / n_trials) / np.log(2)\n",
    "    return cv_bit_trial\n",
    "\n",
    "\n",
    "def create_cv_frame_for_plotting(cv_file):\n",
    "    cvbt_folds_model = load_cv_arr(cv_file)\n",
    "    glm_lapse_model = cvbt_folds_model[:3, ]\n",
    "    idx = np.array([0, 3, 4, 5, 6])\n",
    "    cvbt_folds_model = cvbt_folds_model[idx, :]\n",
    "    # Identify best cvbt:\n",
    "    mean_cvbt = np.mean(cvbt_folds_model, axis=1)\n",
    "    loc_best = np.where(mean_cvbt == max(mean_cvbt))[0]\n",
    "    best_val = max(mean_cvbt)\n",
    "    # Create dataframe for plotting\n",
    "    num_models = cvbt_folds_model.shape[0]\n",
    "    num_folds = cvbt_folds_model.shape[1]\n",
    "    # Create pandas dataframe:\n",
    "    data_for_plotting_df = pd.DataFrame({\n",
    "        'model':\n",
    "            np.repeat(np.arange(num_models), num_folds),\n",
    "        'cv_bit_trial':\n",
    "            cvbt_folds_model.flatten()\n",
    "    })\n",
    "    return data_for_plotting_df, loc_best, best_val, glm_lapse_model\n",
    "\n",
    "\n",
    "def get_file_name_for_best_model_fold(cvbt_folds_model, K, overall_dir,\n",
    "                                      best_init_cvbt_dict):\n",
    "    '''\n",
    "    Get the file name for the best initialization for the K value specified\n",
    "    :param cvbt_folds_model:\n",
    "    :param K:\n",
    "    :param models:\n",
    "    :param overall_dir:\n",
    "    :param best_init_cvbt_dict:\n",
    "    :return:\n",
    "    '''\n",
    "    # Identify best fold for best model:\n",
    "    # loc_best = K - 1\n",
    "    loc_best = 0\n",
    "    best_fold = np.where(cvbt_folds_model[loc_best, :] == max(cvbt_folds_model[\n",
    "                                                              loc_best, :]))[\n",
    "        0][0]\n",
    "    base_path = overall_dir + '/GLM_HMM_K_' + str(K) + '/fold_' + str(\n",
    "        best_fold)\n",
    "    key_for_dict = '/GLM_HMM_K_' + str(K) + '/fold_' + str(best_fold)\n",
    "    best_iter = best_init_cvbt_dict[key_for_dict]\n",
    "    raw_file = base_path + '/iter_' + str(\n",
    "        best_iter) + '/glm_hmm_raw_parameters_itr_' + str(best_iter) + '.npz'\n",
    "    return raw_file\n",
    "\n",
    "\n",
    "def permute_transition_matrix(transition_matrix, permutation):\n",
    "    transition_matrix = transition_matrix[np.ix_(permutation, permutation)]\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def calculate_state_permutation(hmm_params):\n",
    "    '''\n",
    "    If K = 3, calculate the permutation that results in states being ordered\n",
    "    as engaged/bias left/bias right\n",
    "    Else: order states so that they are ordered by engagement\n",
    "    :param hmm_params:\n",
    "    :return: permutation\n",
    "    '''\n",
    "    # GLM weights (note: we have to take negative, because we are interested\n",
    "    # in weights corresponding to p(y = 1) = 1/(1+e^(-w.x)), but returned\n",
    "    # weights from\n",
    "    # code are w such that p(y = 1) = e(w.x)/1+e(w.x))\n",
    "    glm_weights = -hmm_params[2]\n",
    "    K = glm_weights.shape[0]\n",
    "    if K == 3:\n",
    "        # want states ordered as engaged/bias left/bias right\n",
    "        M = glm_weights.shape[2] - 1\n",
    "        # bias coefficient is last entry in dimension 2\n",
    "        engaged_loc = \\\n",
    "            np.where((glm_weights[:, 0, 0] == max(glm_weights[:, 0, 0])))[0][0]\n",
    "        reduced_weights = np.copy(glm_weights)\n",
    "        # set row in reduced weights corresponding to engaged to have a bias\n",
    "        # that will not cause it to have largest bias\n",
    "        reduced_weights[engaged_loc, 0, M] = max(glm_weights[:, 0, M]) - 0.001\n",
    "        bias_left_loc = \\\n",
    "            np.where(\n",
    "                (reduced_weights[:, 0, M] == min(reduced_weights[:, 0, M])))[\n",
    "                0][0]\n",
    "        state_order = [engaged_loc, bias_left_loc]\n",
    "        bias_right_loc = np.arange(3)[np.where(\n",
    "            [range(3)[i] not in state_order for i in range(3)])][0]\n",
    "        permutation = np.array([engaged_loc, bias_left_loc, bias_right_loc])\n",
    "    elif K == 4:\n",
    "        # want states ordered as engaged/bias left/bias right\n",
    "        M = glm_weights.shape[2] - 1\n",
    "        # bias coefficient is last entry in dimension 2\n",
    "        engaged_loc = \\\n",
    "            np.where((glm_weights[:, 0, 0] == max(glm_weights[:, 0, 0])))[0][0]\n",
    "        reduced_weights = np.copy(glm_weights)\n",
    "        # set row in reduced weights corresponding to engaged to have a bias\n",
    "        # that will not\n",
    "        reduced_weights[engaged_loc, 0, M] = max(glm_weights[:, 0, M]) - 0.001\n",
    "        bias_right_loc = \\\n",
    "            np.where(\n",
    "                (reduced_weights[:, 0, M] == max(reduced_weights[:, 0, M])))[\n",
    "                0][0]\n",
    "        bias_left_loc = \\\n",
    "            np.where(\n",
    "                (reduced_weights[:, 0, M] == min(reduced_weights[:, 0, M])))[\n",
    "                0][0]\n",
    "        state_order = [engaged_loc, bias_left_loc, bias_right_loc]\n",
    "        other_loc = np.arange(4)[np.where(\n",
    "            [range(4)[i] not in state_order for i in range(4)])][0]\n",
    "        permutation = np.array(\n",
    "            [engaged_loc, bias_left_loc, bias_right_loc, other_loc])\n",
    "    else:\n",
    "        # order states by engagement: with the most engaged being first.\n",
    "        # Note: argsort sorts inputs from smallest to largest (hence why we\n",
    "        # convert to -ve glm_weights)\n",
    "        permutation = np.argsort(-glm_weights[:, 0, 0])\n",
    "    # assert that all indices are present in permutation exactly once:\n",
    "    assert len(permutation) == K, \"permutation is incorrect size\"\n",
    "    assert check_all_indices_present(permutation, K), \"not all indices \" \\\n",
    "                                                      \"present in \" \\\n",
    "                                                      \"permutation: \" \\\n",
    "                                                      \"permutation = \" + \\\n",
    "                                                      str(permutation)\n",
    "    return permutation\n",
    "\n",
    "\n",
    "def check_all_indices_present(permutation, K):\n",
    "    for i in range(K):\n",
    "        if i not in permutation:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_marginal_posterior(inputs, datas, masks, hmm_params, K, permutation):\n",
    "    # Run forward algorithm on hmm with these parameters and collect gammas:\n",
    "    M = inputs[0].shape[1]\n",
    "    D = datas[0].shape[1]\n",
    "    this_hmm = ssm.HMM(K, D, M,\n",
    "                       observations=\"input_driven_obs\",\n",
    "                       observation_kwargs=dict(C=2),\n",
    "                       transitions=\"standard\")\n",
    "    this_hmm.params = hmm_params\n",
    "    # Get expected states:\n",
    "    expectations = [this_hmm.expected_states(data=data, input=input,\n",
    "                                             mask=np.expand_dims(mask,\n",
    "                                                                 axis=1))[0]\n",
    "                    for data, input, mask\n",
    "                    in zip(datas, inputs, masks)]\n",
    "    # Convert this now to one array:\n",
    "    posterior_probs = np.concatenate(expectations, axis=0)\n",
    "    posterior_probs = posterior_probs[:, permutation]\n",
    "    return posterior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "K_vals = [2, 3, 4, 5]\n",
    "num_folds = 5\n",
    "N_initializations = 20\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cluster_job_arr = []\n",
    "    for K in K_vals:\n",
    "        for i in range(num_folds):\n",
    "            for j in range(N_initializations):\n",
    "                cluster_job_arr.append([K, i, j])\n",
    "    np.savez('../../int-brain-lab/glm-hmm/data/ibl/data_for_cluster/cluster_job_arr.npz',\n",
    "             cluster_job_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference with K = 2; Fold = 0; Iter = 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44291/3805832835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m                        \u001b[0mglobal_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                        \u001b[0minit_param_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                        save_directory)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_44291/3235021578.py\u001b[0m in \u001b[0;36mlaunch_glm_hmm_job\u001b[0;34m(inpt, y, session, mask, session_fold_lookup_table, K, D, C, N_em_iters, transition_alpha, prior_sigma, fold, iter, global_fit, init_param_file, save_directory)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                                  \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_this_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                  \u001b[0msession\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_this_fold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                                                  \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_this_fold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Only do this so that errors are avoided - these y values will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# actually be used for anything (due to violation mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import autograd.numpy as np\n",
    "#from glm_hmm_utils import load_cluster_arr, load_session_fold_lookup, \\\n",
    "#    load_data, create_violation_mask, launch_glm_hmm_job\n",
    "\n",
    "D = 1  # data (observations) dimension\n",
    "C = 2  # number of output types/categories\n",
    "N_em_iters = 1 # 300  # number of EM iterations\n",
    "\n",
    "USE_CLUSTER = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../../int-brain-lab/glm-hmm/data/ibl/data_for_cluster/'\n",
    "    results_dir = '../../int-brain-lab/glm-hmm/results/ibl_global_fit/'\n",
    "\n",
    "    if USE_CLUSTER:\n",
    "        z = int(sys.argv[1])\n",
    "    else:\n",
    "        z = 0\n",
    "\n",
    "    num_folds = 5\n",
    "    global_fit = True\n",
    "    # perform mle => set transition_alpha to 1\n",
    "    transition_alpha = 1\n",
    "    prior_sigma = 100\n",
    "\n",
    "    # Load external files:\n",
    "    cluster_arr_file = data_dir + 'cluster_job_arr.npz'\n",
    "    # Load cluster array job parameters:\n",
    "    cluster_arr = load_cluster_arr(cluster_arr_file)\n",
    "    [K, fold, iter] = cluster_arr[z]\n",
    "\n",
    "    #  read in data and train/test split\n",
    "    animal_file = data_dir + 'all_animals_concat.npz'\n",
    "    session_fold_lookup_table = load_session_fold_lookup(\n",
    "        data_dir + 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "    inpt, y, session = load_data(animal_file)\n",
    "    #  append a column of ones to inpt to represent the bias covariate:\n",
    "    inpt = np.hstack((inpt, np.ones((len(inpt),1))))\n",
    "    y = y.astype('int')\n",
    "    # Identify violations for exclusion:\n",
    "    violation_idx = np.where(y == -1)[0]\n",
    "    nonviolation_idx, mask = create_violation_mask(violation_idx,\n",
    "                                                   inpt.shape[0])\n",
    "\n",
    "    #  GLM weights to use to initialize GLM-HMM\n",
    "    init_param_file = results_dir + '/GLM/fold_' + str(\n",
    "        fold) + '/variables_of_interest_iter_0.npz'\n",
    "\n",
    "    # create save directory for this initialization/fold combination:\n",
    "    save_directory = results_dir + '/GLM_HMM_K_' + str(\n",
    "        K) + '/' + 'fold_' + str(fold) + '/' + '/iter_' + str(iter) + '/'\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    launch_glm_hmm_job(inpt,\n",
    "                       y,\n",
    "                       session,\n",
    "                       maks,\n",
    "                       session_fold_lookup_table,\n",
    "                       K,\n",
    "                       D,\n",
    "                       C,\n",
    "                       N_em_iters,\n",
    "                       transition_alpha,\n",
    "                       prior_sigma,\n",
    "                       fold,\n",
    "                       iter,\n",
    "                       global_fit,\n",
    "                       init_param_file,\n",
    "                       save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_glm_hmm(datas, inputs, masks, K, D, M, C, N_em_iters,\n",
    "                transition_alpha, prior_sigma, global_fit,\n",
    "                params_for_initialization, save_title):\n",
    "    '''\n",
    "    Instantiate and fit GLM-HMM model\n",
    "    :param datas:\n",
    "    :param inputs:\n",
    "    :param masks:\n",
    "    :param K:\n",
    "    :param D:\n",
    "    :param M:\n",
    "    :param C:\n",
    "    :param N_em_iters:\n",
    "    :param global_fit:\n",
    "    :param glm_vectors:\n",
    "    :param save_title:\n",
    "    :return:\n",
    "    '''\n",
    "    if global_fit == True:\n",
    "        # Prior variables\n",
    "        # Choice of prior\n",
    "        this_hmm = ssm.HMM(K,\n",
    "                           D,\n",
    "                           M,\n",
    "                           observations=\"input_driven_obs\",\n",
    "                           observation_kwargs=dict(C=C,\n",
    "                                                   prior_sigma=prior_sigma),\n",
    "                           transitions=\"sticky\",\n",
    "                           transition_kwargs=dict(alpha=transition_alpha,\n",
    "                                                  kappa=0))\n",
    "        # Initialize observation weights as GLM weights with some noise:\n",
    "        glm_vectors_repeated = np.tile(params_for_initialization, (K, 1, 1))\n",
    "        glm_vectors_with_noise = glm_vectors_repeated + np.random.normal(\n",
    "            0, 0.2, glm_vectors_repeated.shape)\n",
    "        this_hmm.observations.params = glm_vectors_with_noise\n",
    "    else:\n",
    "        # Choice of prior\n",
    "        this_hmm = ssm.HMM(K,\n",
    "                           D,\n",
    "                           M,\n",
    "                           observations=\"input_driven_obs\",\n",
    "                           observation_kwargs=dict(C=C,\n",
    "                                                   prior_sigma=prior_sigma),\n",
    "                           transitions=\"sticky\",\n",
    "                           transition_kwargs=dict(alpha=transition_alpha,\n",
    "                                                  kappa=0))\n",
    "        # Initialize HMM-GLM with global parameters:\n",
    "        this_hmm.params = params_for_initialization\n",
    "        # Get log_prior of transitions:\n",
    "    print(\"=== fitting GLM-HMM ========\")\n",
    "    sys.stdout.flush()\n",
    "    # Fit this HMM and calculate marginal likelihood\n",
    "    lls = this_hmm.fit(datas,\n",
    "                       inputs=inputs,\n",
    "                       masks=None,\n",
    "                       method=\"em\",\n",
    "                       num_iters=N_em_iters,\n",
    "                       initialize=False,\n",
    "                       tolerance=10 ** -4)\n",
    "    # Save raw parameters of HMM, as well as loglikelihood during training\n",
    "    np.savez(save_title, this_hmm.params, lls)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of size num_models x num_folds containing\n",
    "# normalized loglikelihood for both train and test splits\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "#from post_processing_utils import load_data, load_session_fold_lookup, \\\n",
    "#    prepare_data_for_cv, calculate_baseline_test_ll, \\\n",
    "#    calculate_glm_test_loglikelihood, calculate_cv_bit_trial, \\\n",
    "#    return_glmhmm_nll, return_lapse_nll\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../../int-brain-lab/glm-hmm/data/ibl/data_for_cluster/'\n",
    "    results_dir = '../../int-brain-lab/glm-hmm/results/ibl_global_fit/'\n",
    "\n",
    "    # Load data\n",
    "    inpt, y, session = load_data(data_dir + 'all_animals_concat.npz')\n",
    "    session_fold_lookup_table = load_session_fold_lookup(\n",
    "        data_dir + 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "    # Parameters\n",
    "    C = 2  # number of output classes\n",
    "    num_folds = 5  # number of folds\n",
    "    D = 1  # number of output dimensions\n",
    "    K_max = 5  # maximum number of latent states\n",
    "    num_models = K_max + 2  # model for each latent + 2 lapse models\n",
    "\n",
    "    animal_preferred_model_dict = {}\n",
    "    models = [\"GLM\", \"Lapse_Model\", \"GLM_HMM\"]\n",
    "\n",
    "    cvbt_folds_model = np.zeros((num_models, num_folds))\n",
    "    cvbt_train_folds_model = np.zeros((num_models, num_folds))\n",
    "\n",
    "    # Save best initialization for each model-fold combination\n",
    "    best_init_cvbt_dict = {}\n",
    "    for fold in range(num_folds):\n",
    "        test_inpt, test_y, test_nonviolation_mask, this_test_session, \\\n",
    "        train_inpt, train_y, train_nonviolation_mask, this_train_session, M,\\\n",
    "        n_test, n_train = prepare_data_for_cv(\n",
    "            inpt, y, session, session_fold_lookup_table, fold)\n",
    "        ll0 = calculate_baseline_test_ll(\n",
    "            train_y[train_nonviolation_mask == 1, :],\n",
    "            test_y[test_nonviolation_mask == 1, :], C)\n",
    "        ll0_train = calculate_baseline_test_ll(\n",
    "            train_y[train_nonviolation_mask == 1, :],\n",
    "            train_y[train_nonviolation_mask == 1, :], C)\n",
    "        for model in models:\n",
    "            print(\"model = \" + str(model))\n",
    "            if model == \"GLM\":\n",
    "                # Load parameters and instantiate a new GLM object with\n",
    "                # these parameters\n",
    "                glm_weights_file = results_dir + '/GLM/fold_' + str(\n",
    "                    fold) + '/variables_of_interest_iter_0.npz'\n",
    "                ll_glm = calculate_glm_test_loglikelihood(\n",
    "                    glm_weights_file, test_y[test_nonviolation_mask == 1, :],\n",
    "                    test_inpt[test_nonviolation_mask == 1, :], M, C)\n",
    "                ll_glm_train = calculate_glm_test_loglikelihood(\n",
    "                    glm_weights_file, train_y[train_nonviolation_mask == 1, :],\n",
    "                    train_inpt[train_nonviolation_mask == 1, :], M, C)\n",
    "                cvbt_folds_model[0, fold] = calculate_cv_bit_trial(\n",
    "                    ll_glm, ll0, n_test)\n",
    "                cvbt_train_folds_model[0, fold] = calculate_cv_bit_trial(\n",
    "                    ll_glm_train, ll0_train, n_train)\n",
    "            elif model == \"Lapse_Model\":\n",
    "                # One lapse parameter model:\n",
    "                cvbt_folds_model[1, fold], cvbt_train_folds_model[\n",
    "                    1,\n",
    "                    fold], _, _ = return_lapse_nll(inpt, y, session,\n",
    "                                                   session_fold_lookup_table,\n",
    "                                                   fold, 1, results_dir, C)\n",
    "                # Two lapse parameter model:\n",
    "                cvbt_folds_model[2, fold], cvbt_train_folds_model[\n",
    "                    2,\n",
    "                    fold], _, _ = return_lapse_nll(inpt, y, session,\n",
    "                                                   session_fold_lookup_table,\n",
    "                                                   fold, 2, results_dir, C)\n",
    "            elif model == \"GLM_HMM\":\n",
    "                for K in range(2, K_max + 1):\n",
    "                    print(\"K = \" + str(K))\n",
    "                    model_idx = 3 + (K - 2)\n",
    "                    cvbt_folds_model[model_idx, fold], \\\n",
    "                    cvbt_train_folds_model[\n",
    "                        model_idx, fold], _, _, init_ordering_by_train = \\\n",
    "                        return_glmhmm_nll(\n",
    "                            np.hstack((inpt, np.ones((len(inpt), 1)))), y,\n",
    "                            session, session_fold_lookup_table, fold,\n",
    "                            K, D, C, results_dir)\n",
    "                    # Save best initialization to dictionary for later:\n",
    "                    key_for_dict = '/GLM_HMM_K_' + str(K) + '/fold_' + str(\n",
    "                        fold)\n",
    "                    best_init_cvbt_dict[key_for_dict] = int(\n",
    "                        init_ordering_by_train[0])\n",
    "    # Save best initialization directories across animals, folds and models\n",
    "    # (only GLM-HMM):\n",
    "    print(cvbt_folds_model)\n",
    "    print(cvbt_train_folds_model)\n",
    "    json_dump = json.dumps(best_init_cvbt_dict)\n",
    "    f = open(results_dir + \"/best_init_cvbt_dict.json\", \"w\")\n",
    "    f.write(json_dump)\n",
    "    f.close()\n",
    "    # Save cvbt_folds_model as numpy array for easy parsing across all\n",
    "    # models and folds\n",
    "    np.savez(results_dir + \"/cvbt_folds_model.npz\", cvbt_folds_model)\n",
    "    np.savez(results_dir + \"/cvbt_train_folds_model.npz\",\n",
    "             cvbt_train_folds_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters from IBL global fits (for K = 2 to 5) to initialize\n",
    "# each animal's model\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from post_processing_utils import load_glmhmm_data, load_cv_arr, \\\n",
    "    create_cv_frame_for_plotting, get_file_name_for_best_model_fold, \\\n",
    "    permute_transition_matrix, calculate_state_permutation\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_dir = '../../data/ibl/data_for_cluster/'\n",
    "    results_dir = '../../results/ibl_global_fit/'\n",
    "    save_directory = data_dir + \"best_global_params/\"\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    labels_for_plot = ['stim', 'pc', 'wsls', 'bias']\n",
    "\n",
    "    cv_file = results_dir + \"/cvbt_folds_model.npz\"\n",
    "    cvbt_folds_model = load_cv_arr(cv_file)\n",
    "\n",
    "    for K in range(2, 6):\n",
    "        print(\"K = \" + str(K))\n",
    "        with open(results_dir + \"/best_init_cvbt_dict.json\", 'r') as f:\n",
    "            best_init_cvbt_dict = json.load(f)\n",
    "\n",
    "        # Get the file name corresponding to the best initialization for\n",
    "        # given K value\n",
    "        raw_file = get_file_name_for_best_model_fold(\n",
    "            cvbt_folds_model, K, results_dir, best_init_cvbt_dict)\n",
    "        hmm_params, lls = load_glmhmm_data(raw_file)\n",
    "\n",
    "        # Calculate permutation\n",
    "        permutation = calculate_state_permutation(hmm_params)\n",
    "        print(permutation)\n",
    "\n",
    "        # Save parameters for initializing individual fits\n",
    "        weight_vectors = hmm_params[2][permutation]\n",
    "        log_transition_matrix = permute_transition_matrix(\n",
    "            hmm_params[1][0], permutation)\n",
    "        init_state_dist = hmm_params[0][0][permutation]\n",
    "        params_for_individual_initialization = [[init_state_dist],\n",
    "                                                [log_transition_matrix],\n",
    "                                                weight_vectors]\n",
    "\n",
    "        np.savez(\n",
    "            save_directory + 'best_params_K_' + str(K) + '.npz',\n",
    "            params_for_individual_initialization)\n",
    "\n",
    "        # Plot these too:\n",
    "        cols = [\"#e74c3c\", \"#15b01a\", \"#7e1e9c\", \"#3498db\", \"#f97306\"]\n",
    "        fig = plt.figure(figsize=(4 * 8, 10),\n",
    "                         dpi=80,\n",
    "                         facecolor='w',\n",
    "                         edgecolor='k')\n",
    "        plt.subplots_adjust(left=0.1,\n",
    "                            bottom=0.24,\n",
    "                            right=0.95,\n",
    "                            top=0.7,\n",
    "                            wspace=0.8,\n",
    "                            hspace=0.5)\n",
    "        plt.subplot(1, 3, 1)\n",
    "        M = weight_vectors.shape[2] - 1\n",
    "        for k in range(K):\n",
    "            plt.plot(range(M + 1),\n",
    "                     -weight_vectors[k][0],\n",
    "                     marker='o',\n",
    "                     label='State ' + str(k + 1),\n",
    "                     color=cols[k],\n",
    "                     lw=4)\n",
    "        plt.xticks(list(range(0, len(labels_for_plot))),\n",
    "                   labels_for_plot,\n",
    "                   rotation='20',\n",
    "                   fontsize=24)\n",
    "        plt.yticks(fontsize=30)\n",
    "        plt.legend(fontsize=30)\n",
    "        plt.axhline(y=0, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "        # plt.ylim((-3, 14))\n",
    "        plt.ylabel(\"Weight\", fontsize=30)\n",
    "        plt.xlabel(\"Covariate\", fontsize=30, labelpad=20)\n",
    "        plt.title(\"GLM Weights: Choice = R\", fontsize=40)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        transition_matrix = np.exp(log_transition_matrix)\n",
    "        plt.imshow(transition_matrix, vmin=0, vmax=1)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            for j in range(transition_matrix.shape[1]):\n",
    "                text = plt.text(j,\n",
    "                                i,\n",
    "                                np.around(transition_matrix[i, j],\n",
    "                                          decimals=3),\n",
    "                                ha=\"center\",\n",
    "                                va=\"center\",\n",
    "                                color=\"k\",\n",
    "                                fontsize=30)\n",
    "        plt.ylabel(\"Previous State\", fontsize=30)\n",
    "        plt.xlabel(\"Next State\", fontsize=30)\n",
    "        plt.xlim(-0.5, K - 0.5)\n",
    "        plt.ylim(-0.5, K - 0.5)\n",
    "        plt.xticks(range(0, K), ('1', '2', '3', '4', '4', '5', '6', '7',\n",
    "                                 '8', '9', '10')[:K],\n",
    "                   fontsize=30)\n",
    "        plt.yticks(range(0, K), ('1', '2', '3', '4', '4', '5', '6', '7',\n",
    "                                 '8', '9', '10')[:K],\n",
    "                   fontsize=30)\n",
    "        plt.title(\"Retrieved\", fontsize=40)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        cols = [\n",
    "            \"#7e1e9c\", \"#0343df\", \"#15b01a\", \"#bf77f6\", \"#95d0fc\",\n",
    "            \"#96f97b\"\n",
    "        ]\n",
    "        cv_file = results_dir + \"/cvbt_folds_model.npz\"\n",
    "        data_for_plotting_df, loc_best, best_val, glm_lapse_model = \\\n",
    "            create_cv_frame_for_plotting(\n",
    "            cv_file)\n",
    "        cv_file_train = results_dir + \"/cvbt_train_folds_model.npz\"\n",
    "        train_data_for_plotting_df, train_loc_best, train_best_val, \\\n",
    "        train_glm_lapse_model = create_cv_frame_for_plotting(\n",
    "            cv_file_train)\n",
    "\n",
    "        glm_lapse_model_cvbt_means = np.mean(glm_lapse_model, axis=1)\n",
    "        train_glm_lapse_model_cvbt_means = np.mean(train_glm_lapse_model,\n",
    "                                                   axis=1)\n",
    "        g = sns.lineplot(\n",
    "            data_for_plotting_df['model'],\n",
    "            data_for_plotting_df['cv_bit_trial'],\n",
    "            err_style=\"bars\",\n",
    "            mew=0,\n",
    "            color=cols[0],\n",
    "            marker='o',\n",
    "            ci=68,\n",
    "            label=\"test\",\n",
    "            alpha=1,\n",
    "            lw=4)\n",
    "        sns.lineplot(\n",
    "            train_data_for_plotting_df['model'],\n",
    "            train_data_for_plotting_df['cv_bit_trial'],\n",
    "            err_style=\"bars\",\n",
    "            mew=0,\n",
    "            color=cols[1],\n",
    "            marker='o',\n",
    "            ci=68,\n",
    "            label=\"train\",\n",
    "            alpha=1,\n",
    "            lw=4)\n",
    "        plt.xlabel(\"Model\", fontsize=30)\n",
    "        plt.ylabel(\"Normalized LL\", fontsize=30)\n",
    "        plt.xticks([0, 1, 2, 3, 4],\n",
    "                   ['1 State', '2 State', '3 State', '4 State', '5 State'],\n",
    "                   rotation=45,\n",
    "                   fontsize=24)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.axhline(y=glm_lapse_model_cvbt_means[2],\n",
    "                    color=cols[2],\n",
    "                    label=\"Lapse (test)\",\n",
    "                    alpha=0.9,\n",
    "                    lw=4)\n",
    "        plt.legend(loc='upper right', fontsize=30)\n",
    "        plt.tick_params(axis='y')\n",
    "        plt.yticks([0.2, 0.3, 0.4, 0.5], fontsize=30)\n",
    "        plt.ylim((0.2, 0.55))\n",
    "        plt.title(\"Model Comparison\", fontsize=40)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fig.savefig(results_dir + 'best_params_cross_validation_K_' +\n",
    "                    str(K) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
